{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "00-populate_basic_graph.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r52bvlYKlYvX"
      },
      "source": [
        "\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell:"
      ],
      "id": "r52bvlYKlYvX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3BVn5xblYva"
      },
      "source": [
        "# !pip install py2neo\n",
        "# !pip install wikipedia\n",
        "# !pip install spacy==3.0.3\n",
        "# !pip install Wikipedia-API"
      ],
      "id": "k3BVn5xblYva",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JClrVyxblYvc",
        "outputId": "31e71c40-1bb3-42b7-f567-b23817e6fa67"
      },
      "source": [
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "id": "JClrVyxblYvc",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMQH_nPlYve"
      },
      "source": [
        "# Configure spacy\n",
        "\n",
        "Prior to actually using spacy, we need to load in some models.  The basic model is their small core library, taken from the web: `en_core_web_sm`, which provides good, basic functionality with a small download size (< 20 MB).  However, one drawback of this basic model is that it doesn't have full word vectors.  Instead, it comes with context-sensitive tensors.  You can still do things like text similarity with it, but if you want to use spacy to create good word vectors, you should use a larger model such as `en_core_web_md` or`en_core_web_lg` since the small models are not known for accuracy.  You can also use a variety of third-party models, but that is beyond the scope of this workshop.  Again, choose the model that works best with your setup.\n",
        "\n",
        "In general, to load the models we use the following command:\n",
        "\n",
        "`python3 -m spacy download en_core_web_md`\n",
        "\n",
        "This has already been completed in the container, but if you want to add other models you can do this either as a cell in this notebook or via the CLI.\n",
        "\n",
        "## API key for Google Knowledge Graph\n",
        "\n",
        "See below for instructions on how to create this key.  When you have the key, save it to a file called `.api_key` in this directory.  If you are doing this on Google Colab, mount your local Google Drive directory and store it there."
      ],
      "id": "7tMQH_nPlYve"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Neck1JhlYve"
      },
      "source": [
        "## If you are running this in Google Colab, install the language model using this command:"
      ],
      "id": "2Neck1JhlYve"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD10Oo0_lYvf",
        "outputId": "ade1a74a-1c7c-45fc-c896-54c54bd0c7f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "id": "LD10Oo0_lYvf",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl (47.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.1 MB 3.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (21.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.62.3)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (5.2.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPPUWHxmd3n",
        "outputId": "6db25d2d-de94-4df8-ead0-5c7939ca4d9d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "4gPPUWHxmd3n",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54hCTtQUlYvg",
        "outputId": "d2787188-c331-4b7b-9c29-37fe2479ece1"
      },
      "source": [
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
        "\n",
        "api_key = open('/content/drive/MyDrive/Colab Notebooks/New Text Document (2).txt').read()\n",
        "\n",
        "non_nc = spacy.load('en_core_web_md')\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe('merge_noun_chunks')\n",
        "\n",
        "print(non_nc.pipe_names)\n",
        "print(nlp.pipe_names)"
      ],
      "id": "54hCTtQUlYvg",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
            "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_noun_chunks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Is1mRO1lYvg"
      },
      "source": [
        "# Query Google Knowledge Graph\n",
        "\n",
        "To query the Google Knowledge Graph you will require an API key, which permits you to have 100,000 read calls per day per project for free.  That will be more than sufficient for this workshop.  To obtain your key, follow [these instructions](https://developers.google.com/knowledge-graph/how-tos/authorizing)."
      ],
      "id": "-Is1mRO1lYvg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuUjL3YQlYvh"
      },
      "source": [
        "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
        "    \n",
        "    text_ls = []\n",
        "    node_label_ls = []\n",
        "    url_ls = []\n",
        "    \n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'indent': indent,\n",
        "        'key': api_key,\n",
        "    }   \n",
        "    \n",
        "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "    response = json.loads(urllib.request.urlopen(url).read())\n",
        "    \n",
        "    if return_lists:\n",
        "        for element in response['itemListElement']:\n",
        "\n",
        "            try:\n",
        "                node_label_ls.append(element['result']['@type'])\n",
        "            except:\n",
        "                node_label_ls.append('')\n",
        "\n",
        "            try:\n",
        "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
        "            except:\n",
        "                text_ls.append('')\n",
        "                \n",
        "            try:\n",
        "                url_ls.append(element['result']['detailedDescription']['url'])\n",
        "            except:\n",
        "                url_ls.append('')\n",
        "                \n",
        "        return text_ls, node_label_ls, url_ls\n",
        "    \n",
        "    else:\n",
        "        return response"
      ],
      "id": "zuUjL3YQlYvh",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8fKnDD5lYvi"
      },
      "source": [
        "# Helper functions for text cleaning\n",
        "\n",
        "As with any data science project, we have a lot of cleaning to do!  These next functions do things like removing \n",
        "\n",
        "- special characters (`remove_special_characters`)\n",
        "- stop words and punctuation (`remove_stop_words_and_punct`)\n",
        "- dates (`remove_dates`)\n",
        "- duplicates (`remove_duplicates`)\n",
        "  - Duplicates crop up many times during this process and we will be battling them a lot!\n",
        "  \n",
        "Then, we can get to the heart of the matter, `create_svo_triples`."
      ],
      "id": "S8fKnDD5lYvi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTR-NMd3lYvk"
      },
      "source": [
        "def remove_special_characters(text):\n",
        "    \n",
        "    regex = re.compile(r'[\\n\\r\\t]')\n",
        "    clean_text = regex.sub(\" \", text)\n",
        "    \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_stop_words_and_punct(text, print_text=False):\n",
        "    \n",
        "    result_ls = []\n",
        "    rsw_doc = non_nc(text)\n",
        "    \n",
        "    for token in rsw_doc:\n",
        "        if print_text:\n",
        "            print(token, token.is_stop)\n",
        "            print('--------------')\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            result_ls.append(str(token))\n",
        "    \n",
        "    result_str = ' '.join(result_ls)\n",
        "\n",
        "    return result_str\n",
        "\n",
        "\n",
        "def create_svo_lists(doc, print_lists):\n",
        "    \n",
        "    subject_ls = []\n",
        "    verb_ls = []\n",
        "    object_ls = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in SUBJECTS:\n",
        "            subject_ls.append((token.lower_, token.idx))\n",
        "        elif token.dep_ in VERBS:\n",
        "            verb_ls.append((token.lemma_, token.idx))\n",
        "        elif token.dep_ in OBJECTS:\n",
        "            object_ls.append((token.lower_, token.idx))\n",
        "\n",
        "    if print_lists:\n",
        "        print('SUBJECTS: ', subject_ls)\n",
        "        print('VERBS: ', verb_ls)\n",
        "        print('OBJECTS: ', object_ls)\n",
        "    \n",
        "    return subject_ls, verb_ls, object_ls\n",
        "\n",
        "\n",
        "def remove_duplicates(tup, tup_posn):\n",
        "    \n",
        "    check_val = set()\n",
        "    result = []\n",
        "    \n",
        "    for i in tup:\n",
        "        if i[tup_posn] not in check_val:\n",
        "            result.append(i)\n",
        "            check_val.add(i[tup_posn])\n",
        "            \n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_dates(tup_ls):\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    for entry in tup_ls:\n",
        "        if not entry[2].isdigit():\n",
        "            clean_tup_ls.append(entry)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_svo_triples(text, print_lists=False):\n",
        "    \n",
        "    clean_text = remove_special_characters(text)\n",
        "    doc = nlp(clean_text)\n",
        "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
        "    \n",
        "    graph_tup_ls = []\n",
        "    dedup_tup_ls = []\n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for subj in subject_ls: \n",
        "        for obj in object_ls:\n",
        "            \n",
        "            dist_ls = []\n",
        "            \n",
        "            for v in verb_ls:\n",
        "                \n",
        "                # Assemble a list of distances between each object and each verb\n",
        "                dist_ls.append(abs(obj[1] - v[1]))\n",
        "                \n",
        "            # Get the index of the verb with the smallest distance to the object \n",
        "            # and return that verb\n",
        "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
        "            \n",
        "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
        "            # later down in the process to allow for proper sentence recognition.\n",
        "\n",
        "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
        "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
        "            \n",
        "            # Add entries to the graph iff neither subject nor object is blank\n",
        "            if no_sw_subj and no_sw_obj:\n",
        "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
        "                graph_tup_ls.append(tup)\n",
        "        \n",
        "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
        "    \n",
        "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
        "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
        "    \n",
        "    return clean_tup_ls"
      ],
      "id": "GTR-NMd3lYvk",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYrTQRLlYvk"
      },
      "source": [
        "# Let's get started!"
      ],
      "id": "PnYrTQRLlYvk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVbVU6Y60zaf"
      },
      "source": [
        "import wikipediaapi as wkapi\n",
        "wiki_climate = wkapi.Wikipedia(\n",
        "        language='en',\n",
        "        extract_format=wkapi.ExtractFormat.WIKI\n",
        ")"
      ],
      "id": "HVbVU6Y60zaf",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTvwA_JW0ZMh",
        "outputId": "6d3a6931-c36e-4661-fda8-154bd8dbdc6d"
      },
      "source": [
        "my_file = {}\n",
        "for items in wikipedia.search('road safety', results = 2):\n",
        "  my_file[items]=wiki_climate.page(items).summary\n",
        "print(my_file)"
      ],
      "id": "qTvwA_JW0ZMh",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Road traffic safety': 'Road traffic safety refers to the methods and measures used to prevent road users from being killed or seriously injured. Typical road users include pedestrians, cyclists, motorists, vehicle passengers, horse riders, and passengers of on-road public transport (mainly buses and trams).\\nBest practices in modern road safety strategy: \\n\\nThe basic strategy of a Safe System approach is to ensure that in the event of a crash, the impact energies remain below the threshold likely to produce either death or serious injury. This threshold will vary from crash scenario to crash scenario, depending upon the level of protection offered to the road users involved. For example, the chances of survival for an unprotected pedestrian hit by a vehicle diminish rapidly at speeds greater than 30 km/h, whereas for a properly restrained motor vehicle occupant the critical impact speed is 50 km/h (for side impact crashes) and 70 km/h (for head-on crashes).\\nAs sustainable solutions for classes of road safety have not been identified, particularly low-traffic rural and remote roads, a hierarchy of control should be applied, similar to classifications used to improve occupational safety and health. At the highest level is sustainable prevention of serious injury and death crashes, with sustainable requiring all key result areas to be considered. At the second level is real-time risk reduction, which involves providing users at severe risk with a specific warning to enable them to take mitigating action. The third level is about reducing the crash risk which involves applying the road-design standards and guidelines (such as from AASHTO), improving driver behavior and enforcement.Traffic safety has been studied as a science for more than 75 years.', '2020–21 Road Safety World Series': 'The Road Safety World Series (also known as Unacademy Road Safety World Series for sponsorship reasons), or RSWS, was an Exhibition T20 cricket competition, which featured retired cricketers and was organised by the Road Safety Cell of Maharashtra to raise awareness about road safety. The 2020–21 edition of the series featured notable retired players from India, England, Sri Lanka, West Indies, South Africa, Australia and Bangladesh. Sunil Gavaskar, former India captain, was the commissioner of the series, while Sachin Tendulkar being its brand ambassador. Ravi Gaikwad is the founder of the Road Safety World Series.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUf4XADhsEQa"
      },
      "source": [
        "# \"\"\"Opening the json file\"\"\"\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/sample(1).json') as d:\n",
        "#     my_file = json.load(d)\n",
        "# print(my_file)"
      ],
      "id": "rUf4XADhsEQa",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3dV5ICWlYvk",
        "outputId": "83568687-5e16-43b2-b00a-7d1aa3c17084"
      },
      "source": [
        "text= \" \".join([text for text in my_file.values()])\n",
        "type(text)"
      ],
      "id": "l3dV5ICWlYvk",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2-X6zbelYvl"
      },
      "source": [
        "# More helper functions\n",
        "\n",
        "Now that we have our text clean, we are ready to start getting information about all of the objects in the list (`get_obj_properties`) such as:\n",
        "\n",
        "- any identified node labels\n",
        "- any descriptions\n",
        "- any URLs\n",
        "\n",
        "These will be used to create node properties for all of the objects.  We end this section with allowing the ML models build into spacy to create word vector embeddings for each node based on the node description.  (If no description is provided, this will be an array of zeros.)"
      ],
      "id": "i2-X6zbelYvl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuKHPz_HlYvl"
      },
      "source": [
        "def get_obj_properties(tup_ls):\n",
        "    \n",
        "    init_obj_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "\n",
        "        try:\n",
        "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
        "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
        "        except:\n",
        "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
        "        \n",
        "        init_obj_tup_ls.append(new_tup)\n",
        "        \n",
        "    return init_obj_tup_ls\n",
        "\n",
        "\n",
        "def add_layer(tup_ls):\n",
        "\n",
        "    svo_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        \n",
        "        if tup[3]:\n",
        "            svo_tup = create_svo_triples(tup[3])\n",
        "            svo_tup_ls.extend(svo_tup)\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    return get_obj_properties(svo_tup_ls)\n",
        "        \n",
        "\n",
        "def subj_equals_obj(tup_ls):\n",
        "    \n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[0] != tup[2]:\n",
        "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
        "            \n",
        "    return new_tup_ls\n",
        "\n",
        "\n",
        "def check_for_string_labels(tup_ls):\n",
        "    # This is for an edge case where the object does not get fully populated\n",
        "    # resulting in the node labels being assigned to string instead of list.\n",
        "    # This may not be strictly necessary and the lines using it are commnted out\n",
        "    # below.  Run this function if you come across this case.\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        if isinstance(el[2], list):\n",
        "            clean_tup_ls.append(el)\n",
        "            \n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_word_vectors(tup_ls):\n",
        "\n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            doc = nlp(tup[3])\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
        "        else:\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
        "        new_tup_ls.append(new_tup)\n",
        "        \n",
        "    return new_tup_ls"
      ],
      "id": "UuKHPz_HlYvl",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ov4gTClYvm"
      },
      "source": [
        "# Let's now run this step by step..."
      ],
      "id": "N5ov4gTClYvm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6RhupTglYvm"
      },
      "source": [
        "# %%time\n",
        "initial_tup_ls = create_svo_triples(text, print_lists=False)"
      ],
      "id": "F6RhupTglYvm",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbGvjlIlYvn",
        "outputId": "b150e862-095b-4743-8c13-df0a427db8d5"
      },
      "source": [
        "initial_tup_ls[0:3]"
      ],
      "id": "GNbGvjlIlYvn",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('road traffic safety', 'refer', 'methods'),\n",
              " ('road traffic safety', 'refer', 'road users'),\n",
              " ('road traffic safety', 'include', 'pedestrians')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhpXl8UvlYvo",
        "outputId": "cbce5faf-8ce3-4ace-e86d-afdc9717e860"
      },
      "source": [
        "%%time\n",
        "init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
        "new_layer_ls = add_layer(init_obj_tup_ls)\n",
        "starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
        "edge_ls = subj_equals_obj(starter_edge_ls)\n",
        "#clean_edge_ls = check_for_string_labels(edge_ls)\n",
        "#clean_edge_ls[0:3]\n",
        "clean_edge_ls = edge_ls"
      ],
      "id": "OhpXl8UvlYvo",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.09 s, sys: 163 ms, total: 8.25 s\n",
            "Wall time: 17.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU6GK6mGlYvp",
        "outputId": "9d0b0956-a583-4b7f-db1a-340a54149707"
      },
      "source": [
        "edge_ls[0:3]"
      ],
      "id": "xU6GK6mGlYvp",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('road traffic safety',\n",
              "  'refer',\n",
              "  'methods',\n",
              "  'Methods is a peer-reviewed scientific journal covering research on techniques in the experimental biological and medical sciences.\\nIt absorbed two journals, ImmunoMethods and NeuroProtocols.',\n",
              "  ['Periodical', 'Thing'],\n",
              "  'https://en.wikipedia.org/wiki/Methods_(journal)'),\n",
              " ('road traffic safety', 'refer', 'road users', [], [], []),\n",
              " ('road traffic safety', 'include', 'pedestrians', '', ['Thing', 'Event'], '')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAyt1etGlYvp",
        "outputId": "c7565371-38e6-4570-d29f-b5fbd2721e0f"
      },
      "source": [
        "%%time\n",
        "edges_word_vec_ls = create_word_vectors(edge_ls)"
      ],
      "id": "TAyt1etGlYvp",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.45 s, sys: 11.1 ms, total: 2.46 s\n",
            "Wall time: 2.47 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h38orUBBlYvp"
      },
      "source": [
        "# Create the node and edge lists to populate the graph with the below helper functions\n",
        "\n",
        "These functions achieve the following:\n",
        "\n",
        "1. Deduping of the node list (`dedup`)\n",
        "2. Under the idea that we might want to use word embeddings, we are pulling them in as a node property.  However, Neo4j doesn't work well with numpy arrays, so we convert that array to a list of floats (`convert_vec_to_ls`).\n",
        "3. Add nodes to the graph with the `py2neo` bulk importer (`add_nodes`)\n",
        "4. Add edges (relationships) to the graph (`add_edges`)"
      ],
      "id": "h38orUBBlYvp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S6pRocklYvp"
      },
      "source": [
        "def dedup(tup_ls):\n",
        "    \n",
        "    visited = set()\n",
        "    output_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if not tup[0] in visited:\n",
        "            visited.add(tup[0])\n",
        "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
        "            \n",
        "    return output_ls\n",
        "\n",
        "\n",
        "def convert_vec_to_ls(tup_ls):\n",
        "    \n",
        "    vec_to_ls_tup = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        vec_ls = [float(v) for v in el[4]]\n",
        "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
        "        vec_to_ls_tup.append(tup)\n",
        "        \n",
        "    return vec_to_ls_tup\n",
        "\n",
        "\n",
        "def add_nodes(tup_ls):   \n",
        "\n",
        "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
        "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
        "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
        "    \n",
        "    return"
      ],
      "id": "0S6pRocklYvp",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stB_0e7olYvq"
      },
      "source": [
        "def add_edges(edge_ls):\n",
        "    \n",
        "    edge_dc = {} \n",
        "    \n",
        "    # Group tuple by verb\n",
        "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
        "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
        "    \n",
        "    for tup in edge_ls: \n",
        "        if tup[1] in edge_dc: \n",
        "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
        "        else: \n",
        "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
        "    \n",
        "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
        "        \n",
        "        tx = graph.begin()\n",
        "        \n",
        "        for el in tup_ls:\n",
        "            source_node = nodes_matcher.match(name=el[0]).first()\n",
        "            target_node = nodes_matcher.match(name=el[2]).first()\n",
        "            if not source_node:\n",
        "                source_node = Node('Node', name=el[0])\n",
        "                tx.create(source_node)\n",
        "            if not target_node:\n",
        "                try:\n",
        "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
        "                    tx.create(target_node)\n",
        "                except:\n",
        "                    continue\n",
        "            try:\n",
        "                rel = Relationship(source_node, edge_labels, target_node)\n",
        "            except:\n",
        "                continue\n",
        "            tx.create(rel)\n",
        "        tx.commit()\n",
        "    \n",
        "    return"
      ],
      "id": "stB_0e7olYvq",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c914H1E0lYvr"
      },
      "source": [
        "# Creating some lists of tuples representing the node and edge lists\n",
        "\n",
        "For our node list, we begin with the query subject (Barack Obama), which is in `edge_ls[0][0]` and put this in the variable `orig_node_tup_ls`.  We assume a node label of `Subject` and no description or word vector.  We then add all of the objects associated with the `edges_word_vec_ls` (`obj_node_tup_ls`) and combine that with the previous variable to create `full_node_tup_ls`, which we then dedupe into `dedup_node_tup_ls`."
      ],
      "id": "c914H1E0lYvr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbVRQP6ilYvr"
      },
      "source": [
        "orig_node_tup_ls = [(edge_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
        "obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
        "full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
        "dedup_node_tup_ls = dedup(full_node_tup_ls)"
      ],
      "id": "FbVRQP6ilYvr",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlP8uP6glYvr",
        "outputId": "805a5b32-47c6-4239-bc69-44fd3b40d5df"
      },
      "source": [
        "len(full_node_tup_ls), len(dedup_node_tup_ls)"
      ],
      "id": "tlP8uP6glYvr",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(240, 215)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xi09YK-lYvr"
      },
      "source": [
        "# Create the node list that will be used to populate the graph..."
      ],
      "id": "3xi09YK-lYvr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_WtnaVZlYvs"
      },
      "source": [
        "node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)"
      ],
      "id": "Q_WtnaVZlYvs",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reXhVDa4lYvs"
      },
      "source": [
        "# Populate the graph\n",
        "\n",
        "Here we establish a connection to the database, which is running on the internal Docker network called `neo4j`.  We provide it the user name and password and also create a class for matching nodes (used when we establish the edges in the graph).  Finally, we add the nodes and edges to populate the database."
      ],
      "id": "reXhVDa4lYvs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCIv-L4olYvs"
      },
      "source": [
        "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
        "# If you are using a Docker container for your DB, use the uncommented line.\n",
        "# graph = Graph(\"bolt://some_ip_address:7687\", name=\"neo4j\", password=\"some_password\")\n",
        "\n",
        "graph = Graph(\"bolt://34.234.167.13:7687\", name=\"neo4j\", password=\"interchanges-brother-notes\")\n",
        "nodes_matcher = NodeMatcher(graph)"
      ],
      "id": "mCIv-L4olYvs",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goLVvcJmlYvs",
        "outputId": "db697b62-8b20-403a-cdf1-a739005a8a81"
      },
      "source": [
        "add_nodes(node_tup_ls)"
      ],
      "id": "goLVvcJmlYvs",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph:  263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrDL4cZSlYvt",
        "outputId": "d549603b-9dac-4a1f-dd9b-c59162a28f5b"
      },
      "source": [
        "add_edges(edges_word_vec_ls)"
      ],
      "id": "lrDL4cZSlYvt",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
            "100%|██████████| 35/35 [00:29<00:00,  1.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XULC5N2QpesO"
      },
      "source": [
        ""
      ],
      "id": "XULC5N2QpesO",
      "execution_count": null,
      "outputs": []
    }
  ]
}