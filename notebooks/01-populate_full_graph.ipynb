{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "01-populate_full_graph.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkX7EyjUpDos"
      },
      "source": [
        "# Let's now cut to the chase on Method 1\n",
        "\n",
        "We are going to skip ahead to the \"meat\" since the majority of the notebook before then is identical to `00`.  If you are running this on Google Colab, be sure to run the following cell:"
      ],
      "id": "MkX7EyjUpDos"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZhMpRx2pDo0",
        "outputId": "1e35be83-36ca-442b-d9c5-f297ee1040a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install py2neo\n",
        "!pip install wikipedia\n",
        "!pip install spacy==3.0.3\n",
        "!pip install Wikipedia-API"
      ],
      "id": "UZhMpRx2pDo0",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wikipedia-API\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Wikipedia-API) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Wikipedia-API) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Wikipedia-API) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Wikipedia-API) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Wikipedia-API) (3.0.4)\n",
            "Building wheels for collected packages: Wikipedia-API\n",
            "  Building wheel for Wikipedia-API (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Wikipedia-API: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=1aff0b7132511da3fc0b92a3466c11027cbe3ac9fef7a8a8656d26dbfb76dfd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built Wikipedia-API\n",
            "Installing collected packages: Wikipedia-API\n",
            "Successfully installed Wikipedia-API-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5kLvRrHpDo3",
        "outputId": "b4306487-8a11-40e7-f137-96a6bf9182e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "id": "k5kLvRrHpDo3",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ZJvnIBpDo6"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We are now going to create a slightly larger graph, this time including Michelle Obama (who should obviously be there, but didn't happen to be part of Barack Obama's Wikipedia summary).  \n",
        "\n",
        "I am going to skip some of the notebook comments from the previous notebook and just include all of the functions we used then at the top of this notebook.  I am also going to create a few helper functions (the last cell of functions before the fun begins) that will make the code a bit \"easier on the eyes.\"  So please feel free to skip on ahead to the bottom of the notebook, knowing that the cell here with all of the functions is the concatenation of all of those functions from the previous notebook.\n",
        "\n",
        "## If you are running this on Google Colab, don't forget to import the language model in the following cell:"
      ],
      "id": "52ZJvnIBpDo6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahtQ33a2pDo8",
        "outputId": "e52a4be4-147f-437c-ee72-cb7580857979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "id": "ahtQ33a2pDo8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl (47.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.1 MB 3.1 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.10)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (5.2.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zddFGO3spbL3",
        "outputId": "5c58a87c-9009-4afa-a405-a1efc600757d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "id": "zddFGO3spbL3",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyHS1aoRpDo8",
        "outputId": "18b9a7b5-6102-45fa-87fc-9c9c4c2e1de3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
        "\n",
        "api_key = open('/content/drive/MyDrive/Colab Notebooks/New Text Document (2).txt').read()\n",
        "\n",
        "non_nc = spacy.load('en_core_web_md')\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe('merge_noun_chunks')"
      ],
      "id": "qyHS1aoRpDo8",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function spacy.pipeline.functions.merge_noun_chunks>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz5QKKDVpDo9"
      },
      "source": [
        "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
        "    \n",
        "    text_ls = []\n",
        "    node_label_ls = []\n",
        "    url_ls = []\n",
        "    \n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'indent': indent,\n",
        "        'key': api_key,\n",
        "    }   \n",
        "    \n",
        "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "    response = json.loads(urllib.request.urlopen(url).read())\n",
        "    \n",
        "    if return_lists:\n",
        "        for element in response['itemListElement']:\n",
        "\n",
        "            try:\n",
        "                node_label_ls.append(element['result']['@type'])\n",
        "            except:\n",
        "                node_label_ls.append('')\n",
        "\n",
        "            try:\n",
        "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
        "            except:\n",
        "                text_ls.append('')\n",
        "                \n",
        "            try:\n",
        "                url_ls.append(element['result']['detailedDescription']['url'])\n",
        "            except:\n",
        "                url_ls.append('')\n",
        "                \n",
        "        return text_ls, node_label_ls, url_ls\n",
        "    \n",
        "    else:\n",
        "        return response"
      ],
      "id": "fz5QKKDVpDo9",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vek7pbd8pDo-"
      },
      "source": [
        "def remove_special_characters(text):\n",
        "    \n",
        "    regex = re.compile(r'[\\n\\r\\t]')\n",
        "    clean_text = regex.sub(\" \", text)\n",
        "    \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_stop_words_and_punct(text, print_text=False):\n",
        "    \n",
        "    result_ls = []\n",
        "    rsw_doc = non_nc(text)\n",
        "    \n",
        "    for token in rsw_doc:\n",
        "        if print_text:\n",
        "            print(token, token.is_stop)\n",
        "            print('--------------')\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            result_ls.append(str(token))\n",
        "    \n",
        "    result_str = ' '.join(result_ls)\n",
        "\n",
        "    return result_str\n",
        "\n",
        "\n",
        "def create_svo_lists(doc, print_lists):\n",
        "    \n",
        "    subject_ls = []\n",
        "    verb_ls = []\n",
        "    object_ls = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in SUBJECTS:\n",
        "            subject_ls.append((token.lower_, token.idx))\n",
        "        elif token.dep_ in VERBS:\n",
        "            verb_ls.append((token.lemma_, token.idx))\n",
        "        elif token.dep_ in OBJECTS:\n",
        "            object_ls.append((token.lower_, token.idx))\n",
        "\n",
        "    if print_lists:\n",
        "        print('SUBJECTS: ', subject_ls)\n",
        "        print('VERBS: ', verb_ls)\n",
        "        print('OBJECTS: ', object_ls)\n",
        "    \n",
        "    return subject_ls, verb_ls, object_ls\n",
        "\n",
        "\n",
        "def remove_duplicates(tup, tup_posn):\n",
        "    \n",
        "    check_val = set()\n",
        "    result = []\n",
        "    \n",
        "    for i in tup:\n",
        "        if i[tup_posn] not in check_val:\n",
        "            result.append(i)\n",
        "            check_val.add(i[tup_posn])\n",
        "            \n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_dates(tup_ls):\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    for entry in tup_ls:\n",
        "        if not entry[2].isdigit():\n",
        "            clean_tup_ls.append(entry)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_svo_triples(text, print_lists=False):\n",
        "    \n",
        "    clean_text = remove_special_characters(text)\n",
        "    doc = nlp(clean_text)\n",
        "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
        "    \n",
        "    graph_tup_ls = []\n",
        "    dedup_tup_ls = []\n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for subj in subject_ls: \n",
        "        for obj in object_ls:\n",
        "            \n",
        "            dist_ls = []\n",
        "            \n",
        "            for v in verb_ls:\n",
        "                \n",
        "                # Assemble a list of distances between each object and each verb\n",
        "                dist_ls.append(abs(obj[1] - v[1]))\n",
        "                \n",
        "            # Get the index of the verb with the smallest distance to the object \n",
        "            # and return that verb\n",
        "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
        "            \n",
        "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
        "            # later down in the process to allow for proper sentence recognition.\n",
        "\n",
        "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
        "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
        "            \n",
        "            # Add entries to the graph iff neither subject nor object is blank\n",
        "            if no_sw_subj and no_sw_obj:\n",
        "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
        "                graph_tup_ls.append(tup)\n",
        "        \n",
        "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
        "    \n",
        "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
        "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
        "    \n",
        "    return clean_tup_ls"
      ],
      "id": "vek7pbd8pDo-",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq9HR5i3pDo_"
      },
      "source": [
        "def get_obj_properties(tup_ls):\n",
        "    \n",
        "    init_obj_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "\n",
        "        try:\n",
        "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
        "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
        "        except:\n",
        "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
        "        \n",
        "        init_obj_tup_ls.append(new_tup)\n",
        "        \n",
        "    return init_obj_tup_ls\n",
        "\n",
        "\n",
        "def add_layer(tup_ls):\n",
        "\n",
        "    svo_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        \n",
        "        if tup[3]:\n",
        "            svo_tup = create_svo_triples(tup[3])\n",
        "            svo_tup_ls.extend(svo_tup)\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    return get_obj_properties(svo_tup_ls)\n",
        "        \n",
        "\n",
        "def subj_equals_obj(tup_ls):\n",
        "    \n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[0] != tup[2]:\n",
        "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
        "            \n",
        "    return new_tup_ls\n",
        "\n",
        "\n",
        "def check_for_string_labels(tup_ls):\n",
        "    # This is for an edge case where the object does not get fully populated\n",
        "    # resulting in the node labels being assigned to string instead of list.\n",
        "    # This may not be strictly necessary and the lines using it are commnted out\n",
        "    # below.  Run this function if you come across this case.\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        if isinstance(el[2], list):\n",
        "            clean_tup_ls.append(el)\n",
        "            \n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_word_vectors(tup_ls):\n",
        "\n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            doc = nlp(tup[3])\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
        "        else:\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
        "        new_tup_ls.append(new_tup)\n",
        "        \n",
        "    return new_tup_ls"
      ],
      "id": "kq9HR5i3pDo_",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJk8v6SMpDpB"
      },
      "source": [
        "def dedup(tup_ls):\n",
        "    \n",
        "    visited = set()\n",
        "    output_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if not tup[0] in visited:\n",
        "            visited.add(tup[0])\n",
        "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
        "            \n",
        "    return output_ls\n",
        "\n",
        "\n",
        "def convert_vec_to_ls(tup_ls):\n",
        "    \n",
        "    vec_to_ls_tup = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        vec_ls = [float(v) for v in el[4]]\n",
        "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
        "        vec_to_ls_tup.append(tup)\n",
        "        \n",
        "    return vec_to_ls_tup\n",
        "\n",
        "\n",
        "def add_nodes(tup_ls):   \n",
        "\n",
        "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
        "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
        "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
        "    \n",
        "    return"
      ],
      "id": "tJk8v6SMpDpB",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfZSAipepDpC"
      },
      "source": [
        "def add_edges(edge_ls):\n",
        "    \n",
        "    edge_dc = {} \n",
        "    \n",
        "    # Group tuple by verb\n",
        "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
        "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
        "    \n",
        "    for tup in edge_ls: \n",
        "        if tup[1] in edge_dc: \n",
        "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
        "        else: \n",
        "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
        "    \n",
        "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
        "        \n",
        "        tx = graph.begin()\n",
        "        \n",
        "        for el in tup_ls:\n",
        "            source_node = nodes_matcher.match(name=el[0]).first()\n",
        "            target_node = nodes_matcher.match(name=el[2]).first()\n",
        "            if not source_node:\n",
        "                source_node = Node('Node', name=el[0])\n",
        "                tx.create(source_node)\n",
        "            if not target_node:\n",
        "                try:\n",
        "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
        "                    tx.create(target_node)\n",
        "                except:\n",
        "                    continue\n",
        "            try:\n",
        "                rel = Relationship(source_node, edge_labels, target_node)\n",
        "            except:\n",
        "                continue\n",
        "            tx.create(rel)\n",
        "        tx.commit()\n",
        "    \n",
        "    return"
      ],
      "id": "RfZSAipepDpC",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C9W_hzvpDpD"
      },
      "source": [
        "# Jump back in here!\n",
        "\n",
        "Now we are going to create the nodes and edges for Climate and Human Health and populate the database with them.\n",
        "\n",
        "To do so, I have combined several of the steps in the previous notebook into a couple of helper functions."
      ],
      "id": "3C9W_hzvpDpD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHH3fGKRpDpD"
      },
      "source": [
        "def edge_tuple_creation(text):\n",
        "    \n",
        "    initial_tup_ls = create_svo_triples(text)\n",
        "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
        "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
        "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
        "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
        "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
        "    \n",
        "    return edges_word_vec_ls\n",
        "\n",
        "\n",
        "def node_tuple_creation(edges_word_vec_ls):\n",
        "    \n",
        "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
        "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
        "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
        "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
        "    #dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
        "    dedup_node_tup_ls = cleaned_node_tup_ls\n",
        "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
        "    \n",
        "    return node_tup_ls    "
      ],
      "id": "cHH3fGKRpDpD",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Y3BeaZpDpD"
      },
      "source": [
        "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
        "# If you are using a Docker container for your DB, use the uncommented line.\n",
        "# graph = Graph(\"bolt://some_ip_address:7687\", name=\"neo4j\", password=\"some_password\")\n",
        "\n",
        "graph = Graph(\"bolt://44.197.216.101:7687\", name=\"neo4j\", password=\"needle-deliveries-adherence\")\n",
        "nodes_matcher = NodeMatcher(graph)"
      ],
      "id": "w8Y3BeaZpDpD",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp7L5vnvtma8"
      },
      "source": [
        "import wikipediaapi as wkapi\n",
        "wiki_climate = wkapi.Wikipedia(\n",
        "        language='en',\n",
        "        extract_format=wkapi.ExtractFormat.WIKI\n",
        ")"
      ],
      "id": "Qp7L5vnvtma8",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHUOfwlCtqvK"
      },
      "source": [
        "climate = {}\n",
        "for items in wikipedia.search('climate change', results = 2):\n",
        "  climate[items]=wiki_climate.page(items).summary\n",
        "\n",
        "health = {}\n",
        "for items in wikipedia.search('human health', results = 2):\n",
        "  health[items]=wiki_climate.page(items).summary  "
      ],
      "id": "HHUOfwlCtqvK",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjpmTfi3pDpE",
        "outputId": "1f22acdd-ead9-468b-8539-8a800e65459b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "climate_text = \" \".join([text for text in climate.values()])\n",
        "climate_edges_word_vec_ls = edge_tuple_creation(climate_text)\n",
        "climate_node_tup_ls = node_tuple_creation(climate_edges_word_vec_ls)\n",
        "\n",
        "health_text = \" \".join([text for text in health.values()])\n",
        "health_edges_word_vec_ls = edge_tuple_creation(health_text)\n",
        "health_node_tup_ls = node_tuple_creation(health_edges_word_vec_ls)"
      ],
      "id": "DjpmTfi3pDpE",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 48s, sys: 916 ms, total: 2min 49s\n",
            "Wall time: 3min 18s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GbkKRJcpDpF",
        "outputId": "f43b9737-4b5d-443f-900e-3cbebecf51e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "full_node_ls = climate_node_tup_ls + health_node_tup_ls\n",
        "full_edge_ls = climate_edges_word_vec_ls + health_edges_word_vec_ls\n",
        "full_dedup_node_tup_ls = dedup(full_node_ls)\n",
        "print(len(full_node_ls), len(full_dedup_node_tup_ls))"
      ],
      "id": "4GbkKRJcpDpF",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "903 700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N35JqRMrpDpG",
        "outputId": "f39c0e2c-3273-4a7c-fc4a-ffa6d2a115ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "add_nodes(full_dedup_node_tup_ls)\n",
        "add_edges(full_edge_ls)"
      ],
      "id": "N35JqRMrpDpG",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph:  700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/110 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
            "100%|██████████| 110/110 [02:11<00:00,  1.19s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ne3JzfpDpH"
      },
      "source": [
        "# Next steps...\n",
        "\n",
        "We now have a graph that we can tinker with.  I would encourage you to explore the Cypher queries that can be found in `cypher_queries/`.  You will need to run several of these in order to use the next notebook, where we will explore doing some machine learning on the graph.  Note that any Cypher query can be done using `py2neo` or the official Neo4j python driver.  In the workshop, same are demonstrated using the Neo4j browser, which you will find at `localhost:7474`."
      ],
      "id": "h-ne3JzfpDpH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFFnaUpwpDpH"
      },
      "source": [
        ""
      ],
      "id": "MFFnaUpwpDpH",
      "execution_count": null,
      "outputs": []
    }
  ]
}