{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "00-populate_basic_graph.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r52bvlYKlYvX"
      },
      "source": [
        "# Let's get started with Method 1...\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell:"
      ],
      "id": "r52bvlYKlYvX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3BVn5xblYva"
      },
      "source": [
        "!pip install py2neo\n",
        "!pip install wikipedia\n",
        "!pip install spacy==3.0.3\n",
        "!pip install Wikipedia-API"
      ],
      "id": "k3BVn5xblYva",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JClrVyxblYvc",
        "outputId": "a83be58b-deb2-442f-ce84-59be9bdc8a4a"
      },
      "source": [
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "id": "JClrVyxblYvc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMQH_nPlYve"
      },
      "source": [
        "# Configure spacy\n",
        "\n",
        "Prior to actually using spacy, we need to load in some models.  The basic model is their small core library, taken from the web: `en_core_web_sm`, which provides good, basic functionality with a small download size (< 20 MB).  However, one drawback of this basic model is that it doesn't have full word vectors.  Instead, it comes with context-sensitive tensors.  You can still do things like text similarity with it, but if you want to use spacy to create good word vectors, you should use a larger model such as `en_core_web_md` or`en_core_web_lg` since the small models are not known for accuracy.  You can also use a variety of third-party models, but that is beyond the scope of this workshop.  Again, choose the model that works best with your setup.\n",
        "\n",
        "In general, to load the models we use the following command:\n",
        "\n",
        "`python3 -m spacy download en_core_web_md`\n",
        "\n",
        "This has already been completed in the container, but if you want to add other models you can do this either as a cell in this notebook or via the CLI.\n",
        "\n",
        "## API key for Google Knowledge Graph\n",
        "\n",
        "See below for instructions on how to create this key.  When you have the key, save it to a file called `.api_key` in this directory.  If you are doing this on Google Colab, mount your local Google Drive directory and store it there."
      ],
      "id": "7tMQH_nPlYve"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Neck1JhlYve"
      },
      "source": [
        "## If you are running this in Google Colab, install the language model using this command:"
      ],
      "id": "2Neck1JhlYve"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD10Oo0_lYvf"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "id": "LD10Oo0_lYvf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPPUWHxmd3n",
        "outputId": "cdff587c-c8df-496c-d3d0-a32acf9f89e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "4gPPUWHxmd3n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54hCTtQUlYvg",
        "outputId": "28f0b01b-507a-4b99-99ac-932d31b860fe"
      },
      "source": [
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
        "\n",
        "api_key = open('/content/drive/MyDrive/Colab Notebooks/New Text Document (2).txt').read()\n",
        "\n",
        "non_nc = spacy.load('en_core_web_md')\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe('merge_noun_chunks')\n",
        "\n",
        "print(non_nc.pipe_names)\n",
        "print(nlp.pipe_names)"
      ],
      "id": "54hCTtQUlYvg",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
            "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_noun_chunks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Is1mRO1lYvg"
      },
      "source": [
        "# Query Google Knowledge Graph\n",
        "\n",
        "To query the Google Knowledge Graph you will require an API key, which permits you to have 100,000 read calls per day per project for free.  That will be more than sufficient for this workshop.  To obtain your key, follow [these instructions](https://developers.google.com/knowledge-graph/how-tos/authorizing)."
      ],
      "id": "-Is1mRO1lYvg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuUjL3YQlYvh"
      },
      "source": [
        "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
        "    \n",
        "    text_ls = []\n",
        "    node_label_ls = []\n",
        "    url_ls = []\n",
        "    \n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'indent': indent,\n",
        "        'key': api_key,\n",
        "    }   \n",
        "    \n",
        "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "    response = json.loads(urllib.request.urlopen(url).read())\n",
        "    \n",
        "    if return_lists:\n",
        "        for element in response['itemListElement']:\n",
        "\n",
        "            try:\n",
        "                node_label_ls.append(element['result']['@type'])\n",
        "            except:\n",
        "                node_label_ls.append('')\n",
        "\n",
        "            try:\n",
        "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
        "            except:\n",
        "                text_ls.append('')\n",
        "                \n",
        "            try:\n",
        "                url_ls.append(element['result']['detailedDescription']['url'])\n",
        "            except:\n",
        "                url_ls.append('')\n",
        "                \n",
        "        return text_ls, node_label_ls, url_ls\n",
        "    \n",
        "    else:\n",
        "        return response"
      ],
      "id": "zuUjL3YQlYvh",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8fKnDD5lYvi"
      },
      "source": [
        "# Helper functions for text cleaning\n",
        "\n",
        "As with any data science project, we have a lot of cleaning to do!  These next functions do things like removing \n",
        "\n",
        "- special characters (`remove_special_characters`)\n",
        "- stop words and punctuation (`remove_stop_words_and_punct`)\n",
        "- dates (`remove_dates`)\n",
        "- duplicates (`remove_duplicates`)\n",
        "  - Duplicates crop up many times during this process and we will be battling them a lot!\n",
        "  \n",
        "Then, we can get to the heart of the matter, `create_svo_triples`."
      ],
      "id": "S8fKnDD5lYvi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTR-NMd3lYvk"
      },
      "source": [
        "def remove_special_characters(text):\n",
        "    \n",
        "    regex = re.compile(r'[\\n\\r\\t]')\n",
        "    clean_text = regex.sub(\" \", text)\n",
        "    \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_stop_words_and_punct(text, print_text=False):\n",
        "    \n",
        "    result_ls = []\n",
        "    rsw_doc = non_nc(text)\n",
        "    \n",
        "    for token in rsw_doc:\n",
        "        if print_text:\n",
        "            print(token, token.is_stop)\n",
        "            print('--------------')\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            result_ls.append(str(token))\n",
        "    \n",
        "    result_str = ' '.join(result_ls)\n",
        "\n",
        "    return result_str\n",
        "\n",
        "\n",
        "def create_svo_lists(doc, print_lists):\n",
        "    \n",
        "    subject_ls = []\n",
        "    verb_ls = []\n",
        "    object_ls = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in SUBJECTS:\n",
        "            subject_ls.append((token.lower_, token.idx))\n",
        "        elif token.dep_ in VERBS:\n",
        "            verb_ls.append((token.lemma_, token.idx))\n",
        "        elif token.dep_ in OBJECTS:\n",
        "            object_ls.append((token.lower_, token.idx))\n",
        "\n",
        "    if print_lists:\n",
        "        print('SUBJECTS: ', subject_ls)\n",
        "        print('VERBS: ', verb_ls)\n",
        "        print('OBJECTS: ', object_ls)\n",
        "    \n",
        "    return subject_ls, verb_ls, object_ls\n",
        "\n",
        "\n",
        "def remove_duplicates(tup, tup_posn):\n",
        "    \n",
        "    check_val = set()\n",
        "    result = []\n",
        "    \n",
        "    for i in tup:\n",
        "        if i[tup_posn] not in check_val:\n",
        "            result.append(i)\n",
        "            check_val.add(i[tup_posn])\n",
        "            \n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_dates(tup_ls):\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    for entry in tup_ls:\n",
        "        if not entry[2].isdigit():\n",
        "            clean_tup_ls.append(entry)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_svo_triples(text, print_lists=False):\n",
        "    \n",
        "    clean_text = remove_special_characters(text)\n",
        "    doc = nlp(clean_text)\n",
        "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
        "    \n",
        "    graph_tup_ls = []\n",
        "    dedup_tup_ls = []\n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for subj in subject_ls: \n",
        "        for obj in object_ls:\n",
        "            \n",
        "            dist_ls = []\n",
        "            \n",
        "            for v in verb_ls:\n",
        "                \n",
        "                # Assemble a list of distances between each object and each verb\n",
        "                dist_ls.append(abs(obj[1] - v[1]))\n",
        "                \n",
        "            # Get the index of the verb with the smallest distance to the object \n",
        "            # and return that verb\n",
        "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
        "            \n",
        "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
        "            # later down in the process to allow for proper sentence recognition.\n",
        "\n",
        "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
        "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
        "            \n",
        "            # Add entries to the graph iff neither subject nor object is blank\n",
        "            if no_sw_subj and no_sw_obj:\n",
        "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
        "                graph_tup_ls.append(tup)\n",
        "        \n",
        "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
        "    \n",
        "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
        "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
        "    \n",
        "    return clean_tup_ls"
      ],
      "id": "GTR-NMd3lYvk",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYrTQRLlYvk"
      },
      "source": [
        "# Let's get started!"
      ],
      "id": "PnYrTQRLlYvk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVbVU6Y60zaf"
      },
      "source": [
        "import wikipediaapi as wkapi\n",
        "wiki_climate = wkapi.Wikipedia(\n",
        "        language='en',\n",
        "        extract_format=wkapi.ExtractFormat.WIKI\n",
        ")"
      ],
      "id": "HVbVU6Y60zaf",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTvwA_JW0ZMh",
        "outputId": "a817a4f1-a2b8-4313-a421-c5c2017313a9"
      },
      "source": [
        "my_file = {}\n",
        "for items in wikipedia.search('climate change', results = 2):\n",
        "  my_file[items]=wiki_climate.page(items).summary\n",
        "print(my_file)"
      ],
      "id": "qTvwA_JW0ZMh",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Climate change': 'Climate change includes both global warming driven by human-induced emissions of greenhouse gases and the resulting large-scale shifts in weather patterns. Though there have been previous periods of climatic change, since the mid-20th century humans have had an unprecedented impact on Earth\\'s climate system and caused change on a global scale.The largest driver of warming is the emission of gases that create a greenhouse effect, of which more than 90% are carbon dioxide (CO2) and methane. Fossil fuel burning (coal, oil, and natural gas) for energy consumption is the main source of these emissions, with additional contributions from agriculture, deforestation, and the chemical reactions in certain manufacturing processes. The human cause of climate change is not disputed by any scientific body of national or international standing. Temperature rise is amplified by climate feedbacks, such as loss of sunlight-reflecting snow and ice cover, increased water vapour (a greenhouse gas itself), and changes to land and ocean carbon sinks.\\nOn land, where temperatures have risen about twice as fast as the global average, deserts are expanding and heat waves and wildfires are becoming more common. Temperature rise is also amplified in the Arctic, where it has contributed to melting permafrost, glacial retreat and sea ice loss. Warmer temperatures are increasing rates of evaporation, causing more intense storms and weather extremes. Impacts on ecosystems include the relocation or extinction of many species as their environment changes, most immediately in coral reefs, mountains, and the Arctic. Climate change threatens people with food insecurity, water scarcity, flooding, infectious diseases, extreme heat, economic losses, and displacement. These human impacts have led the World Health Organization to call climate change the greatest threat to global health in the 21st century. Even if efforts to minimise future warming are successful, some effects will continue for centuries, including rising sea levels, rising ocean temperatures, and ocean acidification.\\n\\nMany of these impacts are already felt at the current level of warming, which is about 1.2 °C (2.2 °F). The Intergovernmental Panel on Climate Change (IPCC) has issued a series of reports that project significant increases in these impacts as warming continues to 1.5 °C (2.7 °F) and beyond. Additional warming also increases the risk of triggering critical thresholds called tipping points. Responding to these impacts involves both mitigation and adaptation. Mitigation – limiting climate change – consists of reducing greenhouse gas emissions and removing them from the atmosphere. Methods to achieve this include the development and deployment of low-carbon energy sources such as wind and solar, a phase-out of coal, enhanced energy efficiency, and forest preservation. Adaptation consists of adjusting to actual or expected climate, such as through improved coastline protection, better disaster management, and the development of more resistant crops. Adaptation alone cannot avert the risk of \"severe, widespread and irreversible\" impacts.Under the 2015 Paris Agreement, nations collectively agreed to keep warming \"well under 2.0 °C (3.6 °F)\" through mitigation efforts. However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century. Limiting warming to 1.5 °C (2.7 °F) would require halving emissions by 2030 and achieving near-zero emissions by 2050.', 'Climate change denial': 'Climate change denial, or global warming denial, is denial, dismissal, or unwarranted doubt that contradicts the scientific consensus on climate change, including the extent to which it is caused by humans, its effects on nature and human society, or the potential of adaptation to global warming by human actions. Many who deny, dismiss, or hold unwarranted doubt about the scientific consensus on anthropogenic global warming self-label as \"climate change skeptics\", which several scientists have noted is an inaccurate description. Climate change denial can also be implicit when individuals or social groups accept the science but fail to come to terms with it or to translate their acceptance into action. Several social science studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.The campaign to undermine public trust in climate science has been described as a \"denial machine\" organized by industrial, political and ideological interests, and supported by conservative media and skeptical bloggers to manufacture uncertainty about global warming.The politics of global warming have been affected by climate change denial and the political global warming controversy, undermining the efforts to act on climate change or adapting to the warming climate. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none.Organised campaigning to undermine public trust in climate science is associated with conservative economic policies and backed by industrial interests opposed to the regulation of CO2 emissions. Climate change denial has been associated with the fossil fuels lobby, the Koch brothers, industry advocates and conservative think tanks, often in the United States. More than 90% of papers skeptical on climate change originate from right-wing think tanks.Since the late 1970s, oil companies have published research broadly in line with the standard views on global warming. Despite this, oil companies organized a climate change denial campaign to disseminate public disinformation for several decades, a strategy that has been compared to the organized denial of the hazards of tobacco smoking by the tobacco industry, and often even carried out by the same individuals who previously spread the tobacco industry\\'s denialist propaganda.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUf4XADhsEQa"
      },
      "source": [
        "# \"\"\"Opening the json file\"\"\"\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/sample(1).json') as d:\n",
        "#     my_file = json.load(d)\n",
        "# print(my_file)"
      ],
      "id": "rUf4XADhsEQa",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3dV5ICWlYvk",
        "outputId": "71260c5a-db42-4068-a767-3caa97a975f6"
      },
      "source": [
        "text= \" \".join([text for text in my_file.values()])\n",
        "type(text)"
      ],
      "id": "l3dV5ICWlYvk",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2-X6zbelYvl"
      },
      "source": [
        "# More helper functions\n",
        "\n",
        "Now that we have our text clean, we are ready to start getting information about all of the objects in the list (`get_obj_properties`) such as:\n",
        "\n",
        "- any identified node labels\n",
        "- any descriptions\n",
        "- any URLs\n",
        "\n",
        "These will be used to create node properties for all of the objects.  We end this section with allowing the ML models build into spacy to create word vector embeddings for each node based on the node description.  (If no description is provided, this will be an array of zeros.)"
      ],
      "id": "i2-X6zbelYvl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuKHPz_HlYvl"
      },
      "source": [
        "def get_obj_properties(tup_ls):\n",
        "    \n",
        "    init_obj_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "\n",
        "        try:\n",
        "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
        "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
        "        except:\n",
        "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
        "        \n",
        "        init_obj_tup_ls.append(new_tup)\n",
        "        \n",
        "    return init_obj_tup_ls\n",
        "\n",
        "\n",
        "def add_layer(tup_ls):\n",
        "\n",
        "    svo_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        \n",
        "        if tup[3]:\n",
        "            svo_tup = create_svo_triples(tup[3])\n",
        "            svo_tup_ls.extend(svo_tup)\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    return get_obj_properties(svo_tup_ls)\n",
        "        \n",
        "\n",
        "def subj_equals_obj(tup_ls):\n",
        "    \n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[0] != tup[2]:\n",
        "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
        "            \n",
        "    return new_tup_ls\n",
        "\n",
        "\n",
        "def check_for_string_labels(tup_ls):\n",
        "    # This is for an edge case where the object does not get fully populated\n",
        "    # resulting in the node labels being assigned to string instead of list.\n",
        "    # This may not be strictly necessary and the lines using it are commnted out\n",
        "    # below.  Run this function if you come across this case.\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        if isinstance(el[2], list):\n",
        "            clean_tup_ls.append(el)\n",
        "            \n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_word_vectors(tup_ls):\n",
        "\n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            doc = nlp(tup[3])\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
        "        else:\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
        "        new_tup_ls.append(new_tup)\n",
        "        \n",
        "    return new_tup_ls"
      ],
      "id": "UuKHPz_HlYvl",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ov4gTClYvm"
      },
      "source": [
        "# Let's now run this step by step..."
      ],
      "id": "N5ov4gTClYvm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6RhupTglYvm"
      },
      "source": [
        "# %%time\n",
        "initial_tup_ls = create_svo_triples(text, print_lists=False)"
      ],
      "id": "F6RhupTglYvm",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbGvjlIlYvn",
        "outputId": "bf0db241-da0c-40d8-df41-624f93f4c5cb"
      },
      "source": [
        "initial_tup_ls[0:3]"
      ],
      "id": "GNbGvjlIlYvn",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('climate change', 'include', 'global warming'),\n",
              " ('climate change', 'include', 'human induced emissions'),\n",
              " ('climate change', 'include', 'greenhouse gases')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhpXl8UvlYvo",
        "outputId": "49619bfc-966b-4221-aa90-e61ed6d5355c"
      },
      "source": [
        "%%time\n",
        "init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
        "new_layer_ls = add_layer(init_obj_tup_ls)\n",
        "starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
        "edge_ls = subj_equals_obj(starter_edge_ls)\n",
        "#clean_edge_ls = check_for_string_labels(edge_ls)\n",
        "#clean_edge_ls[0:3]\n",
        "clean_edge_ls = edge_ls"
      ],
      "id": "OhpXl8UvlYvo",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 23.9 s, sys: 441 ms, total: 24.4 s\n",
            "Wall time: 2min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU6GK6mGlYvp",
        "outputId": "4f92fd66-92c9-49da-94b2-09b470e024db"
      },
      "source": [
        "edge_ls[0:3]"
      ],
      "id": "xU6GK6mGlYvp",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('climate change',\n",
              "  'include',\n",
              "  'global warming',\n",
              "  'Climate change includes both global warming driven by human-induced emissions of greenhouse gases and the resulting large-scale shifts in weather patterns. ',\n",
              "  ['Thing'],\n",
              "  'https://en.wikipedia.org/wiki/Climate_change'),\n",
              " ('climate change', 'include', 'human induced emissions', [], [], []),\n",
              " ('climate change', 'include', 'greenhouse gases', [], [], [])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAyt1etGlYvp",
        "outputId": "911adc91-eb54-41c5-93df-9fa66eec49ee"
      },
      "source": [
        "%%time\n",
        "edges_word_vec_ls = create_word_vectors(edge_ls)"
      ],
      "id": "TAyt1etGlYvp",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.84 s, sys: 43 ms, total: 6.88 s\n",
            "Wall time: 6.88 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h38orUBBlYvp"
      },
      "source": [
        "# Create the node and edge lists to populate the graph with the below helper functions\n",
        "\n",
        "These functions achieve the following:\n",
        "\n",
        "1. Deduping of the node list (`dedup`)\n",
        "2. Under the idea that we might want to use word embeddings, we are pulling them in as a node property.  However, Neo4j doesn't work well with numpy arrays, so we convert that array to a list of floats (`convert_vec_to_ls`).\n",
        "3. Add nodes to the graph with the `py2neo` bulk importer (`add_nodes`)\n",
        "4. Add edges (relationships) to the graph (`add_edges`)"
      ],
      "id": "h38orUBBlYvp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S6pRocklYvp"
      },
      "source": [
        "def dedup(tup_ls):\n",
        "    \n",
        "    visited = set()\n",
        "    output_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if not tup[0] in visited:\n",
        "            visited.add(tup[0])\n",
        "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
        "            \n",
        "    return output_ls\n",
        "\n",
        "\n",
        "def convert_vec_to_ls(tup_ls):\n",
        "    \n",
        "    vec_to_ls_tup = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        vec_ls = [float(v) for v in el[4]]\n",
        "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
        "        vec_to_ls_tup.append(tup)\n",
        "        \n",
        "    return vec_to_ls_tup\n",
        "\n",
        "\n",
        "def add_nodes(tup_ls):   \n",
        "\n",
        "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
        "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
        "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
        "    \n",
        "    return"
      ],
      "id": "0S6pRocklYvp",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stB_0e7olYvq"
      },
      "source": [
        "def add_edges(edge_ls):\n",
        "    \n",
        "    edge_dc = {} \n",
        "    \n",
        "    # Group tuple by verb\n",
        "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
        "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
        "    \n",
        "    for tup in edge_ls: \n",
        "        if tup[1] in edge_dc: \n",
        "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
        "        else: \n",
        "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
        "    \n",
        "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
        "        \n",
        "        tx = graph.begin()\n",
        "        \n",
        "        for el in tup_ls:\n",
        "            source_node = nodes_matcher.match(name=el[0]).first()\n",
        "            target_node = nodes_matcher.match(name=el[2]).first()\n",
        "            if not source_node:\n",
        "                source_node = Node('Node', name=el[0])\n",
        "                tx.create(source_node)\n",
        "            if not target_node:\n",
        "                try:\n",
        "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
        "                    tx.create(target_node)\n",
        "                except:\n",
        "                    continue\n",
        "            try:\n",
        "                rel = Relationship(source_node, edge_labels, target_node)\n",
        "            except:\n",
        "                continue\n",
        "            tx.create(rel)\n",
        "        tx.commit()\n",
        "    \n",
        "    return"
      ],
      "id": "stB_0e7olYvq",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c914H1E0lYvr"
      },
      "source": [
        "# Creating some lists of tuples representing the node and edge lists\n",
        "\n",
        "For our node list, we begin with the query subject (Barack Obama), which is in `edge_ls[0][0]` and put this in the variable `orig_node_tup_ls`.  We assume a node label of `Subject` and no description or word vector.  We then add all of the objects associated with the `edges_word_vec_ls` (`obj_node_tup_ls`) and combine that with the previous variable to create `full_node_tup_ls`, which we then dedupe into `dedup_node_tup_ls`."
      ],
      "id": "c914H1E0lYvr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbVRQP6ilYvr"
      },
      "source": [
        "orig_node_tup_ls = [(edge_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
        "obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
        "full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
        "dedup_node_tup_ls = dedup(full_node_tup_ls)"
      ],
      "id": "FbVRQP6ilYvr",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlP8uP6glYvr",
        "outputId": "11badc3f-7852-4426-f870-cc89b5f04b11"
      },
      "source": [
        "len(full_node_tup_ls), len(dedup_node_tup_ls)"
      ],
      "id": "tlP8uP6glYvr",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(666, 556)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xi09YK-lYvr"
      },
      "source": [
        "# Create the node list that will be used to populate the graph..."
      ],
      "id": "3xi09YK-lYvr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_WtnaVZlYvs"
      },
      "source": [
        "node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)"
      ],
      "id": "Q_WtnaVZlYvs",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reXhVDa4lYvs"
      },
      "source": [
        "# Populate the graph\n",
        "\n",
        "Here we establish a connection to the database, which is running on the internal Docker network called `neo4j`.  We provide it the user name and password and also create a class for matching nodes (used when we establish the edges in the graph).  Finally, we add the nodes and edges to populate the database."
      ],
      "id": "reXhVDa4lYvs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCIv-L4olYvs"
      },
      "source": [
        "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
        "# If you are using a Docker container for your DB, use the uncommented line.\n",
        "# graph = Graph(\"bolt://some_ip_address:7687\", name=\"neo4j\", password=\"some_password\")\n",
        "\n",
        "graph = Graph(\"bolt://18.212.181.122:7687\", name=\"neo4j\", password=\"human-ride-lightning\")\n",
        "nodes_matcher = NodeMatcher(graph)"
      ],
      "id": "mCIv-L4olYvs",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goLVvcJmlYvs",
        "outputId": "01d19302-f7a7-4302-8a12-c0ee9f92e05c"
      },
      "source": [
        "add_nodes(node_tup_ls)"
      ],
      "id": "goLVvcJmlYvs",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph:  628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrDL4cZSlYvt",
        "outputId": "db9aa110-d6f4-4399-a649-69a7904a3548"
      },
      "source": [
        "add_edges(edges_word_vec_ls)"
      ],
      "id": "lrDL4cZSlYvt",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/87 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
            "100%|██████████| 87/87 [01:20<00:00,  1.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju867sQflYvt"
      },
      "source": [
        "# Entity disambiguation\n",
        "\n",
        "In this notebook we will calculate the cosine similarity of the word vectors of target nodes to try and determine the likelihood of two nodes being the same entity.  Note that you could do this directly with spacy (which defaults to cosine similarity) prior to anything above through something like:\n",
        "\n",
        "```\n",
        "doc1 = nlp(text1)\n",
        "doc2 = nlp(text2)\n",
        "doc1.similarity(doc2)\n",
        "```\n",
        "\n",
        "However, since we have already calculated the word vectors for each node description above, we will just use the cosine similarity built into `scikit-learn`. \n",
        "\n",
        "Note that we are creating now a complete node list since the nodes we are comparing might be either in the Barack or the Michelle Obama node lists..."
      ],
      "id": "ju867sQflYvt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs33y-y1lYvu"
      },
      "source": [
        "def get_word_vec_similarity(node1, node2, node_ls):\n",
        "    \n",
        "    node1_vec = [tup[4] for tup in node_ls if tup[0] == node1]\n",
        "    node2_vec = [tup[4] for tup in node_ls if tup[0] == node2]\n",
        "    \n",
        "    return cosine_similarity(node1_vec, node2_vec)"
      ],
      "id": "Gs33y-y1lYvu",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dkXT1YGlYvv",
        "outputId": "6d1a8db2-40c8-4e99-8622-10540f3564e9"
      },
      "source": [
        "cs = get_word_vec_similarity('climate change', 'global scale', dedup_node_tup_ls)\n",
        "print(cs)"
      ],
      "id": "3dkXT1YGlYvv",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.00475776]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eSfMnHXlYvv"
      },
      "source": [
        "## ...OK, that one was cheating...\n",
        "\n",
        "...because the Google Knowledge Graph knew that these were the same thing.  The NER returned these as being different nodes, but Google knew better and gave the exact description for each.  This will not necessarily be the case in more complicated knowledge graphs."
      ],
      "id": "4eSfMnHXlYvv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3gxo3zSlYvv",
        "outputId": "3612f697-f17c-407b-99a9-5ecdd435570e"
      },
      "source": [
        "cs = get_word_vec_similarity('climate change', 'heating', dedup_node_tup_ls)\n",
        "print(cs)"
      ],
      "id": "f3gxo3zSlYvv",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00147182]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoKIcFVXlYvv"
      },
      "source": [
        "# Problem!\n",
        "\n",
        "There is a key figure missing from this graph!  (Hint: in Cypher, try the following command:\n",
        "\n",
        "```\n",
        "MATCH (n:Node)\n",
        "WHERE n.name CONTAINS 'michelle'\n",
        "RETURN n\n",
        "```\n",
        "\n",
        "In this next notebook, we are going to rectify this and build out the graph a little further."
      ],
      "id": "qoKIcFVXlYvv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBfDfYxQlYvv"
      },
      "source": [
        ""
      ],
      "id": "SBfDfYxQlYvv",
      "execution_count": null,
      "outputs": []
    }
  ]
}